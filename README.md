# Advanced Regression Techniques
დატასეტში არის მოცემული დახასიათება სახლებზე და მათი ფასები. პროექტის მიზანია ისეთი მოდელის შემუშავება, რომელიც ამ სახლების ფასებს მოცემული მახასიათებლებით მაქსიმალურად ზუსტად შეაფასებს

ექსპერიმენტებში ძირითადად შემდეგნაირად მაქვს მოწყობილი: თავდაპირველად ერთნაირად ვამზადებ დატას, ხოლოდ შემდეგ ხელით ვცვლი feature engineering-ისა და selection-ს ლოგიკას, რომელიც წინადგამზადებულ რეგრესიულ მოდელებზე დაეშვება განსხვავებული პარამეტრებით

## რეპოზიტორიის სტრუქტურა
`data/data_description.txt` - დატასეტის დახასიათება

`data/sample_submission.csv` - მაგალითი თუ როგორ უნდა გაიგზავნოს პასუხი

`data/test.csv` - სატესტო დატასეტი

`data/train.csv` - სატრეინინგო დატასეტი

`model_experiment.ipynb` - სადაც ექსპერიმენტებს ვუშვებ და ვამოწმებ

`model_inference.ipynb` - სადაც ვამოწმებ საუკეთესო მოდელს

## Feature Engineering

### კატეგორიული ცვლადებიდან რიცხვითზე გადაყვანა

აქ ვიყენებ 4 მეთოდს:

* Target Encoding - კატეგორიებს ვანაცვლებ ამ კატეგორიის დაჯგუფებით სახლის ფასის საშუალოთი
* Frequency Encoding - ნაცვლდება იმის მიხედვით თუ რამდენად ხშირად გვხვდება კატეგორია დატასეტში
* One Hot Encoding - კატეგორიის ყველა შესაძლო ვარიანტი გადის ცალკე ცვლადად
* Label Encoding - ყველა კატეგორიას ენიჭება რიცხვი დალაგების მიხედვით

ყოველ კატეგორიაზე გადავცემ სიას თუ როგორ უნდა გადავიდეს რიცხვითში. თუ არაფერი გადაეცემა, ჩვეულებრივ ყველა კატეგორიული ცვლადი გადაიყვანება Label Encoding-თ

### Nan მნიშვნელობების დამუშავება + Cleaning

`MSSubClass as category` - რიცხვებით არის ჩაწერილი, მაგრამ სინამდვილეში კატეგორიებია

`Fill LotFrontage NaN with median` - აქ NaN არასრულყოფილ ინფორმაციაზე მიანიშნებს, ამიტომ შეივსება მედიანით

`MasVnrType NaN handling` - თუ NaN არის, ანუ MasVnrArea 0-ს ტოლია

`Electrical NaN handling` - თუ NaN, დიდი ალბათობით ყველაზე გავრცელებული კატეგორია იქნება, რადგან თითქმის ყველა სახლს აქვს რაიმე სახის ელექტროგაყვანილობა

`GarageYrBlt binning` - თუ არ აქვს გარაჟი, ანუ ვერც აგების წელი ექნება. ამიტომაც, უფრო სწორად ჩავთვალე წლები bin-ებში ჩამეყარა და NaN-ები ცალკე, რომ ინფორმაცია მეტ-ნაკლებად სწორად შემენარჩუნებინა

`Fill remaining NaN with 'None'` - დანარჩენი, სადაც NaN მნიშვნელობა არის, None-ებით შევავსე, რადგან კატეგორიული ცვლადები იყო ყველა

## Feature Selection

აქ ვიყენებდი 2 მეთოდს: `Correlation Filter` და `RFE`

ამ 2 მიდგომისთვის მქონდა შესაბამისად 2 პარამეტრი: `corr_threshold` და `rfe_n_features`

ამ 2 ცვლადს ხელით გადავარჩევდი რათა თანდათან მივსულიყავი უკეთეს მოდელამდე

## Training

წინად მოყვანილი ყველა ეტაპის შესრულების შემდეგ დატა გადის 2 შრეში: `scaler` და `regressor`

`scaler` გადაირჩევა 5-ს შორის:

* `StandardScaler` - საშუალო დაყავს 0-ზე და ვარიაცია 1-ზე
* `MinMaxScaler` - ყველა დატა დაყავს \[0,1\] ინტერვალში 
* `RobustScaler` - ისე ასკალირებს რომ outlier-ებს ნაკლებად შეუძლიათ ცვლილება
* `Normalizer` - თითო სტრიქონი რომ ვექტორად გადავიყვანოთ, 1 ზომის უნდა იყოს
* `None` - არანაირი ცვლილება

`regressor` გადაირჩევა 3 მოდელს შორის:

* `LinearRegression` - სწავლობს წონებს ჩვეულებრივად loss ფუნქციის მინიმიზაციით
    * `Fit Intercept` - თუ True, მაშინ საწყისი წერტილი 0-ზე არ უნდა იყოს
* `Ridge` - L2 რეგულარიზაციით დიდი წონებისგან თავს ირიდებს
    * `alpha` - რეგულარიზაციის ძალა
* `Lasso` - L1 რეგულარიზაციით წონების აბსოლუტურ მნიშვნელობებს ამცირებს
    * `alpha` - რეგულარიზაციის ძალა

დატას სიმწირის გამო, ვიყენებ `Cross Validation`-ს, რომელიც 5-fold-ით იყოფა

შესაფასებისთვის ვიყენებ `Negative Mean Square Error`-ს, რადგან მინიმუმი უნდა ვიპოვო

ხელით რომ არ მომწეოდა ამ ყველა მოდელის გადარჩევა, გამოვიყენე `GridSearchCV`, რომელიც ყველა შესაძლო კომბინაციას გადის და საუკეთესო შედეგთან ერთად სხვა მაჩვნებლებსაც აბრუნებს, რათა სხვა მოდელების გარჩევაც მოვახერხო

### initial_mixed

თავდაპირველად გავტესტე რენდომ feature selection-ით, სადაც ბევრი იყო one hot encoding-თ. Selection-ს ბოლოს აღმოჩნდა რომ RFE-მ აარჩია ბევრი ისეთი ცვლადი, რომელიც ohe-თ იყო ენკოდირებული, თუმცა ტესტირების დრო ცუდი შედეგი აჩვენა. დიდი ალბათობი იმიტომ, რომ ეს ენკოდირებული დატა კარგად ერგებოდა ტრეინინგს მხოლოდ, მაგრამ რეალურად სატესტო დატაზე ცუდად მუშაობდა

### full_label_encoding

შემდეგ ყველა კატეგორიული ცვლადი label encoding-ით გადავიყვანე. პირველ პრობლემას გამოასწორებდა, სადაც ohe-თ ბევრი დატა იყო. ამან წინა ექსპერიმენტის შედეგები გააუმჯობესა და overfitting ნაკლებად ჩანდა, მაგრამ სატესტო და სატრეინინგო შედეგები მაინც შესამჩნევად განხვავდებოდა

### full_label_encoding_30

ამჯერად ცვლილება ისაა რომ 30 ცვლადი გამოვიყენე, რომელიც RFE-მ აარჩია. ამან სატესტო შედეგები უფრო გააუმჯობესა და overfitting ნაკლებად ჩანდა

## MLflow Tracking

### MLflow ექსპერიმენტების ბმული

### ჩაწერილი მეტრიკების აღწერა

#### Parameters

* `target_encoded` - რომელი ცვლადები უნდა იყოს target encoded
* `frequency_encoded` - რომელი ცვლადები უნდა იყოს frequency encoded
* `one_hot_encoded` - რომელი ცვლადები უნდა იყოს one hot encoded
* `corr_threshold` - კორელაციის ზღვრები
* `rfe_n_features` - RFE-სთვის რამდენი ცვლადი უნდა იყოს
* `selected_features` - რომელი ცვლადები უნდა იყოს დატასეტში
* `scaler` - რომელი სკალერი უნდა იყოს გამოყენებული
* `regressor` - რომელი მოდელი უნდა იყოს გამოყენებული
* `regressor__fit_intercept` - თუ True, მაშინ საწყისი წერტილი 0-ზე არ უნდა იყოს
* `regressor__alpha` - რეგულარიზაციის ძალა
* `regressor__max_iter` - მაქსიმალური_iterations
* `regressor__tol` - ტოლერანტობა

#### Metrics

* `rmse_train_score` - ტრენინგის შედეგის ფესვი
* `rmse_test_score` - ტესტირების შედეგის ფესვი
* `std_train_score` - ტრენინგის შედეგების სტანდარტული გადახრის ფესვი
* `std_test_score` - ტესტირების შედეგების სტანდარტული გადახრის ფესვი
* `rank_test_score` - ტესტირების შედეგების რეიტინგი მოცემული ექსპერიმენტის სერიაში

### საუკეთესო მოდელის შედეგები

`initial_mixed`

Overfit:
    Initial_Mixed_3(worst) 28 33 5(Underfit) 8 9 6 7(Underfit) 38 4